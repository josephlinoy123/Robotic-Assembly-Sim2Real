# Bridging the Sim-to-Real Gap in Robotic Assembly Using Generative AI and RL

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.12%2B-red.svg)](https://pytorch.org/)

This repository contains the source code and related materials for my master's thesis:  
**"Leveraging Generative AI and Reinforcement Learning to Improve Robot-based Assembly Task Simulations"**  
*Deggendorf Institute of Technology, 2025*

## ğŸ“ Abstract
This repository contains the complete implementation of a novel two-stage approach combining:
1. **VAE-LSTM models** for synthetic force/torque data generation
2. **Proximal Policy Optimization (PPO)** for RL-based control policy training

Key achievements:
- **98.98% MSE improvement** in synthetic force data realism vs physics-based simulation
- Successful sim-to-real transfer learning for robotic assembly tasks
- Custom `RobotEnv` Gym environment for RL training

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- NVIDIA GPU (recommended)

### Installation
```bash
git clone https://github.com/josephlinoy123/Robotic-Assembly-Sim2Real.git
cd Robotic-Assembly-Sim2Real
pip install -r requirements.txt
```

### Dataset Preparation
1. Download [CSIRO Manipulation Benchmark Dataset](https://research.csiro.au/robotics/manipulation-benchmark/)
2. Place raw data in `data/real/` & `data/simulated/`

## ğŸ§  Model Architecture
### Two-Stage Methodology
```mermaid
graph TD
    A[Data Acquisition] --> B[Preprocessing]
    B --> C[Train VAE-LSTM]
    C --> D[Generate Synthetic Data]
    D --> E[RL Training]
    E --> F[Initialize RobotEnv]
    F --> G[Train PPO Agent]
    G --> H{Episode Done?}
    H -->|No| G
    H -->|Yes| I[Save Trained Model]
```    
## ğŸ’» Usage
### 1. Generate Synthetic Data
```bash
# Train ForceVAE model
python src/vae_lstm/train.py --config configs/force_vae.yaml

# Generate synthetic trajectories
python src/vae_lstm/generate_data.py --task 01 --output data/generated/
```

### 2. Train RL Agent
```bash
python src/rl_training/train_ppo.py \
  --env RobotEnv-v1 \
  --total_timesteps 1000000 \
  --log_dir logs/
```

## ğŸ“‚ Folder Structure
```
robot/                   # Project root
â”œâ”€â”€ data/                # RAW data
â”‚   â”œâ”€â”€ real/            # As-is
â”‚   â””â”€â”€ simulated/       # As-is
â”œâ”€â”€ output_split_models/ # Generates outputs
â”‚   â”œâ”€â”€ force/           # Saves .pth files
â”‚   â”œâ”€â”€ torque/          # Saves .pth files
â”‚   â”œâ”€â”€ generated/       # Saves .npy files
â”‚   â”œâ”€â”€ metrics/         # Saves CSVs
â”‚   â”œâ”€â”€ plots/           # Saves PNGs
â”‚   â””â”€â”€ rl_models/       # Saves PPO.zip
â”œâ”€â”€ src/                 # NEW: All executable code
â”‚   â”œâ”€â”€ vae_lstm/        # VAE components
â”‚   â”‚   â”œâ”€â”€ train.py     
â”‚   â”‚   â”œâ”€â”€ generate.py
â”‚   â”‚   â””â”€â”€ models.py
â”‚   â””â”€â”€ rl_training/     # RL components
â”‚       â”œâ”€â”€ train_ppo.py
â”‚       â””â”€â”€ environments.py
```

## ğŸ“š Citation
If you use this work in your research, please cite:
```bibtex
@mastersthesis{Arakkal2025Sim2Real,
  author  = {Joseph Linoy Arakkal},
  title   = {Leveraging Generative AI and Reinforcement Learning to Improve Robot-based Assembly Task Simulations},
  school  = {Deggendorf Institute of Technology},
  year    = {2025},
  url     = {https://github.com/josephlinoy123/Robotic-Assembly-Sim2Real}
}
```

## ğŸ“œ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments
- Supervisors: Mr. Ginu Paul Alunkal and Dr. Alper Yaman
- Dataset providers: CSIRO Robotics
